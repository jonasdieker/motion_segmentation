{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from DatasetClass import CarlaMotionSeg\n",
    "from train import train, run_val\n",
    "from utils_train import get_dataloaders, setup_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Command Line Arguments + Define Setup Function (can be left unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--lr\", default=1.25e-5, type=float, help='Learning rate - default: 5e-3')\n",
    "    parser.add_argument(\"--batch_size\", default=1, type=int, help='Default=2')\n",
    "    parser.add_argument(\"--epochs\", default=50, type=int, help='Default=50')\n",
    "    parser.add_argument(\"--loss_type\", default='focal', type=str, help='Loss types available - focal, bce')\n",
    "    parser.add_argument(\"--patience\", default=3, type=int, help='Default=3')\n",
    "    parser.add_argument(\"--lr_scheduler_factor\", default=0.5, type=float, help=\"Learning rate multiplier - default: 3\")\n",
    "    parser.add_argument(\"--alpha\", default=0.25, type=float, help='Focal loss alpha - default: 0.25')\n",
    "    parser.add_argument(\"--gamma\", default=2.0, type=float, help='Focal loss gamma - default: 2')\n",
    "    parser.add_argument(\"--load_chkpt\", '-chkpt', default='0', type=str, help=\"Loading entire checkpoint path for inference/continue training\")\n",
    "    parser.add_argument(\"--dataset_fraction\", default=0.002, type=float, help=\"fraction of dataset to be used\")\n",
    "    return parser\n",
    "\n",
    "def train_setup(args):\n",
    "    # data_root = os.path.join(root, \"datasets/Extended_MOD_Masks/\")\n",
    "    data_root = os.path.join(args.root, \"datasets/Carla_supervised\")\n",
    "    log_root = os.path.join(args.root, \"logs/\")\n",
    "    root_tb = os.path.join(args.root, \"runs_temp/\")\n",
    "    args.root_tb = root_tb\n",
    "\n",
    "    # define string needed for logging\n",
    "    args.now = datetime.now()\n",
    "    now_string = args.now.strftime(f\"%d-%m-%Y_%H-%M_{args.batch_size}_{args.lr}_{args.epochs}\")\n",
    "    \n",
    "    # setup logging\n",
    "    args, logger = setup_logger(args, log_root, now_string)\n",
    "\n",
    "    # log general info\n",
    "    logger.info(f\"running with lr={args.lr}, batch_size={args.batch_size}, epochs={args.epochs}, loss_type = {args.loss_type}, patience={args.patience}, lr_scheduler_factor={args.lr_scheduler_factor} alpha={args.alpha}, gamma={args.gamma}\")\n",
    "    logger.info(f\"running on '{args.device}'\")\n",
    "\n",
    "    # define dataset and get data loaders\n",
    "    dataset = CarlaMotionSeg(data_root)\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(dataset, args)\n",
    "\n",
    "    # initialize tensorboard\n",
    "    args.writer = SummaryWriter(os.path.join(root_tb, now_string))\n",
    "\n",
    "    return args, logger, train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the root and run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] running with lr=1.25e-05, batch_size=1, epochs=50, loss_type = focal, patience=3, lr_scheduler_factor=0.5 alpha=0.25, gamma=2.0\n",
      "[INFO] running on 'cuda:0'\n",
      "dirs loaded:\n",
      "['0000', '0001', '0002', '0003', '0004', '0005', '0006', '0007', '0008', '0009', '0010', '0011', '0012', '0013', '0014', '0015', '0016', '0017', '0018', '0019', '0020', '0021', '0022', '0023', '0024', '0025']\n",
      "[INFO] loaded model of type: <class 'ModelClass.UNET'>\n",
      "train network ...\n",
      "[INFO] Epoch [1/50] with lr 1.25e-05, train loss: 81970.14648, val loss: 97956.90625, IoU: 0.0, ETA: 0.1 hrs\n",
      "[INFO] Epoch [2/50] with lr 1.25e-05, train loss: 76218.88477, val loss: 100956.625, IoU: 0.0, ETA: 0.08 hrs\n",
      "[INFO] Epoch [3/50] with lr 1.25e-05, train loss: 72142.92578, val loss: 100922.39062, IoU: 0.0, ETA: 0.07 hrs\n",
      "[INFO] Epoch [4/50] with lr 1.25e-05, train loss: 69196.80664, val loss: 97564.75781, IoU: 0.0, ETA: 0.06 hrs\n",
      "[INFO] Epoch [5/50] with lr 1.25e-05, train loss: 66596.42578, val loss: 91396.40625, IoU: 0.0, ETA: 0.06 hrs\n",
      "[INFO] Epoch [6/50] with lr 1.25e-05, train loss: 64060.11426, val loss: 84197.67188, IoU: 0.0, ETA: 0.06 hrs\n",
      "[INFO] Epoch [7/50] with lr 1.25e-05, train loss: 61692.28906, val loss: 77442.96875, IoU: 0.0, ETA: 0.05 hrs\n",
      "[INFO] Epoch [8/50] with lr 1.25e-05, train loss: 59554.90137, val loss: 71565.97656, IoU: 0.0, ETA: 0.05 hrs\n",
      "[INFO] Epoch [9/50] with lr 1.25e-05, train loss: 57472.92285, val loss: 65538.76562, IoU: 0.0, ETA: 0.05 hrs\n",
      "[INFO] Epoch [10/50] with lr 1.25e-05, train loss: 55403.34473, val loss: 58974.30859, IoU: 0.0, ETA: 0.05 hrs\n",
      "[INFO] Epoch 10 Current best IoU at epoch 10\n",
      "[INFO] Epoch [11/50] with lr 1.25e-05, train loss: 53480.24023, val loss: 52896.78516, IoU: 0.0, ETA: 0.05 hrs\n",
      "[INFO] Epoch [12/50] with lr 1.25e-05, train loss: 51697.67773, val loss: 47974.59766, IoU: 0.0, ETA: 0.05 hrs\n",
      "[INFO] Epoch [13/50] with lr 1.25e-05, train loss: 49972.77344, val loss: 44166.49219, IoU: 0.0, ETA: 0.04 hrs\n",
      "[INFO] Epoch [14/50] with lr 1.25e-05, train loss: 48362.8584, val loss: 41414.9375, IoU: 0.0, ETA: 0.04 hrs\n",
      "[INFO] Epoch [15/50] with lr 1.25e-05, train loss: 46840.90723, val loss: 39556.85938, IoU: 0.0, ETA: 0.04 hrs\n",
      "[INFO] Epoch [16/50] with lr 1.25e-05, train loss: 45362.61816, val loss: 38370.79688, IoU: 0.0, ETA: 0.04 hrs\n",
      "[INFO] Epoch [17/50] with lr 1.25e-05, train loss: 43976.65234, val loss: 37590.30469, IoU: 0.0, ETA: 0.04 hrs\n",
      "[INFO] Epoch [18/50] with lr 1.25e-05, train loss: 42629.45508, val loss: 36938.86719, IoU: 0.0, ETA: 0.04 hrs\n",
      "[INFO] Epoch [19/50] with lr 1.25e-05, train loss: 41363.46777, val loss: 36237.14062, IoU: 0.0, ETA: 0.04 hrs\n",
      "[INFO] Epoch [20/50] with lr 1.25e-05, train loss: 40120.95312, val loss: 35598.39062, IoU: 0.0, ETA: 0.03 hrs\n",
      "[INFO] Epoch 20 Current best IoU at epoch 20\n",
      "[INFO] Epoch [21/50] with lr 1.25e-05, train loss: 38940.57617, val loss: 34978.49609, IoU: 0.0, ETA: 0.03 hrs\n",
      "[INFO] Epoch [22/50] with lr 1.25e-05, train loss: 37758.74023, val loss: 34225.60156, IoU: 0.0, ETA: 0.03 hrs\n",
      "[INFO] Epoch [23/50] with lr 1.25e-05, train loss: 36624.07129, val loss: 33477.78125, IoU: 0.0, ETA: 0.03 hrs\n",
      "[INFO] Epoch [24/50] with lr 1.25e-05, train loss: 35554.83008, val loss: 32656.65234, IoU: 0.0, ETA: 0.03 hrs\n",
      "[INFO] Epoch [25/50] with lr 1.25e-05, train loss: 34521.58984, val loss: 31840.30469, IoU: 0.0, ETA: 0.03 hrs\n",
      "[INFO] Epoch [26/50] with lr 1.25e-05, train loss: 33469.14062, val loss: 30938.83008, IoU: 0.0, ETA: 0.03 hrs\n",
      "[INFO] Epoch [27/50] with lr 1.25e-05, train loss: 32464.53613, val loss: 30221.0, IoU: 0.0, ETA: 0.03 hrs\n",
      "[INFO] Epoch [28/50] with lr 1.25e-05, train loss: 31528.97217, val loss: 29314.94922, IoU: 0.0, ETA: 0.03 hrs\n",
      "[INFO] Epoch [29/50] with lr 1.25e-05, train loss: 30621.35742, val loss: 28767.04102, IoU: 0.0, ETA: 0.02 hrs\n",
      "[INFO] Epoch [30/50] with lr 1.25e-05, train loss: 29716.66895, val loss: 27909.14062, IoU: 0.0, ETA: 0.02 hrs\n",
      "[INFO] Epoch 30 Current best IoU at epoch 29\n",
      "[INFO] Epoch [31/50] with lr 1.25e-05, train loss: 29051.48535, val loss: 27916.94922, IoU: 0.0, ETA: 0.02 hrs\n",
      "[INFO] Epoch [32/50] with lr 1.25e-05, train loss: 28357.58008, val loss: 26997.16016, IoU: 0.0, ETA: 0.02 hrs\n",
      "[INFO] Epoch [33/50] with lr 1.25e-05, train loss: 27396.68018, val loss: 26118.40625, IoU: 0.0, ETA: 0.02 hrs\n",
      "[INFO] Epoch [34/50] with lr 1.25e-05, train loss: 26645.03467, val loss: 25489.17188, IoU: 0.0, ETA: 0.02 hrs\n",
      "[INFO] Epoch [35/50] with lr 1.25e-05, train loss: 25885.77197, val loss: 24926.74219, IoU: 0.0, ETA: 0.02 hrs\n",
      "[INFO] Epoch [36/50] with lr 1.25e-05, train loss: 25273.96094, val loss: 24286.10547, IoU: 0.0, ETA: 0.02 hrs\n",
      "[INFO] Epoch [37/50] with lr 1.25e-05, train loss: 24578.91016, val loss: 23629.75, IoU: 0.0, ETA: 0.01 hrs\n",
      "[INFO] Epoch [38/50] with lr 1.25e-05, train loss: 23995.40576, val loss: 23222.16406, IoU: 0.0, ETA: 0.01 hrs\n",
      "[INFO] Epoch [39/50] with lr 1.25e-05, train loss: 23414.04834, val loss: 22778.09375, IoU: 0.0, ETA: 0.01 hrs\n",
      "[INFO] Epoch [40/50] with lr 1.25e-05, train loss: 22847.27734, val loss: 22193.56836, IoU: 0.0, ETA: 0.01 hrs\n",
      "[INFO] Epoch 40 Current best IoU at epoch 39\n",
      "[INFO] Epoch [41/50] with lr 1.25e-05, train loss: 22307.2666, val loss: 21655.44922, IoU: 0.0, ETA: 0.01 hrs\n",
      "[INFO] Epoch [42/50] with lr 1.25e-05, train loss: 21784.17041, val loss: 21220.04492, IoU: 0.0, ETA: 0.01 hrs\n",
      "[INFO] Epoch [43/50] with lr 1.25e-05, train loss: 22365.97119, val loss: 21628.45312, IoU: 0.0, ETA: 0.01 hrs\n",
      "[INFO] Epoch [44/50] with lr 1.25e-05, train loss: 21301.62256, val loss: 24650.07812, IoU: 0.0, ETA: 0.01 hrs\n",
      "[INFO] Epoch [45/50] with lr 1.25e-05, train loss: 22049.68945, val loss: 24591.36914, IoU: 0.0, ETA: 0.01 hrs\n",
      "Epoch    46: reducing learning rate of group 0 to 6.2500e-06.\n",
      "[INFO] Epoch [46/50] with lr 6.25e-06, train loss: 21742.19043, val loss: 23161.75, IoU: 0.0, ETA: 0.0 hrs\n",
      "[INFO] Epoch [47/50] with lr 6.25e-06, train loss: 21226.32373, val loss: 22025.96094, IoU: 0.0, ETA: 0.0 hrs\n",
      "[INFO] Epoch [48/50] with lr 6.25e-06, train loss: 20889.62939, val loss: 21228.33203, IoU: 0.0, ETA: 0.0 hrs\n",
      "[INFO] Epoch [49/50] with lr 6.25e-06, train loss: 20597.14697, val loss: 20645.44336, IoU: 0.0, ETA: 0.0 hrs\n",
      "[INFO] Epoch [50/50] with lr 6.25e-06, train loss: 20303.40186, val loss: 20233.78516, IoU: 0.0, ETA: 0.0 hrs\n",
      "[INFO] Epoch 50 Current best IoU at epoch 42\n"
     ]
    }
   ],
   "source": [
    "args = parse().parse_args(\"\")\n",
    "args.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "args.root = \"/storage/remote/atcremers40/motion_seg/\"\n",
    "\n",
    "args, logger, train_loader, val_loader = train_setup(args)\n",
    "train(args, train_loader, val_loader, None, logger)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1defcf583daa72ff2562ab5259c869074eb4282387c5d4b161f752a4f80423e5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
